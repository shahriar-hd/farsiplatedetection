{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# پروژه تشخیص پلاک خودروهای ایرانی با استفاده از YOLO\n",
    "\n",
    "## نمای کلی پروژه\n",
    "\n",
    "### اهداف پروژه\n",
    "\n",
    "این پروژه با هدف طراحی و پیاده‌سازی یک سیستم تشخیص پلاک خودروهای ایرانی بر روی یک سیستم embedded (رزبری پای) انجام شده است. برخلاف اکثر سیستم‌های موجود که در تشخیص پلاک‌های خاص (مانند پلاک‌های دیپلمات، سرویس، معلولین و ...) با مشکل مواجه هستند، ما سعی کردیم این مشکل را با استفاده از یک رویکرد یکپارچه و افزایش داده‌های هدفمند حل کنیم.\n",
    "\n",
    "### نوآوری‌های پروژه\n",
    "\n",
    "1. **رویکرد یکپارچه**: به جای استفاده از دو مدل جداگانه (یکی برای تشخیص پلاک و دیگری برای تشخیص کاراکترها)، ما از یک مدل YOLO واحد برای همزمان تشخیص محدوده پلاک و تمام کاراکترهای آن استفاده کردیم.\n",
    "\n",
    "2. **پوشش پلاک‌های خاص**: مدل ما قادر به تشخیص 41 کلاس مختلف شامل پلاک، اعداد (0-9)، حروف فارسی و نمادهای خاص است.\n",
    "\n",
    "3. **بهینه‌سازی برای سخت‌افزار محدود**: پیاده‌سازی بر روی Raspberry Pi 5 با بهینه‌سازی‌های لازم برای اجرا با فریم‌ریت قابل قبول.\n",
    "\n",
    "### کلاس‌های تعریف شده در مدل\n",
    "\n",
    "مدل ما شامل 41 کلاس است:\n",
    "- **plate**: ناحیه کلی پلاک\n",
    "- **اعداد**: 0 تا 9\n",
    "- **حروف فارسی**: الف، ب، پ، ت، ث، ج، د، ز، س، ش، ص، ط، ظ، ع، ف، ق، ک، گ، ل، م، ن، و، ه، ی، ژ\n",
    "- **نمادهای خاص**: تاکسی، معلولین، پلیس، دیپلمات (D)، سرویس (S)، تشریفات\n",
    "\n",
    "### مراحل انجام پروژه\n",
    "\n",
    "1. جمع‌آوری و پردازش دیتاست IR-LPR\n",
    "2. تبدیل فرمت annotations از XML به YOLO\n",
    "3. افزایش داده برای کلاس‌های کم‌نمونه (Data Augmentation)\n",
    "4. آموزش مدل YOLOv8n\n",
    "5. ارزیابی و تست مدل\n",
    "6. بهینه‌سازی برای Raspberry Pi (تبدیل به NCNN)\n",
    "7. پیاده‌سازی نهایی با دوربین RPi Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## بخش اول: آماده‌سازی دیتاست\n",
    "\n",
    "### توضیحات\n",
    "\n",
    "دیتاست IR-LPR که از GitHub دریافت شده، شامل بیش از 76,000 تصویر با annotations در فرمت XML است. این دیتاست دارای مشکلات زیر بود:\n",
    "\n",
    "1. **فرمت نامناسب**: YOLO نیاز به فرمت خاص خود (class x_center y_center width height) دارد در حالی که دیتاست در فرمت XML با مختصات (xmin, ymin, xmax, ymax) بود.\n",
    "\n",
    "2. **عدم تعادل کلاس‌ها**: از 76,000 تصویر، کمتر از 3,000 تصویر شامل پلاک‌های خاص (دیپلمات، معلولین، تشریفات و ...) بودند.\n",
    "\n",
    "### راه‌حل\n",
    "\n",
    "دو اسکریپت اصلی برای حل این مشکلات نوشته شد:\n",
    "- **Dataset**: تبدیل فرمت و سازماندهی دیتاست\n",
    "- **Augmentation**: افزایش داده برای کلاس‌های کم‌نمونه\n",
    "\n",
    "### عملکرد dataset\n",
    "\n",
    "این اسکریپت وظایف زیر را انجام می‌دهد:\n",
    "1. خواندن فایل‌های XML از سه پوشه منبع (car_img, plate_img, plate_img_dummy)\n",
    "2. تبدیل مختصات bounding box از فرمت (xmin, ymin, xmax, ymax) به فرمت YOLO نرمالیزه شده\n",
    "3. نگاشت برچسب‌های فارسی به شماره کلاس‌های انگلیسی\n",
    "4. سازماندهی داده‌ها در ساختار استاندارد YOLO:\n",
    "   ```\n",
    "   yolo_dataset/\n",
    "   ├── images/\n",
    "   │   ├── train/\n",
    "   │   ├── val/\n",
    "   │   └── test/\n",
    "   ├── labels/\n",
    "   │   ├── train/\n",
    "   │   ├── val/\n",
    "   │   └── test/\n",
    "   └── data.yaml\n",
    "   ```\n",
    "5. ایجاد فایل data.yaml برای آموزش YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "SOURCE_DIRS = [\n",
    "    \"car_img\",\n",
    "    \"plate_img\",\n",
    "    \"plate_img_dummy\"\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"yolo_dataset\"\n",
    "\n",
    "# Mapping original folder names to YOLO standard splits\n",
    "SPLIT_MAPPING = {\n",
    "    \"train\": \"train\",\n",
    "    \"test\": \"test\",\n",
    "    \"validation\": \"val\"\n",
    "}\n",
    "\n",
    "# Define all 41 classes in English\n",
    "CLASS_NAMES = [\n",
    "    \"plate\",      # Full plate region\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",  # Digits\n",
    "    \"alef\",       # ا\n",
    "    \"be\",         # ب\n",
    "    \"pe\",         # پ\n",
    "    \"te\",         # ت\n",
    "    \"se\",         # ث\n",
    "    \"jim\",        # ج\n",
    "    \"dal\",        # د\n",
    "    \"sin\",        # س\n",
    "    \"shin\",       # ش\n",
    "    \"sad\",        # ص\n",
    "    \"ta\",         # ط\n",
    "    \"za\",         # ظ\n",
    "    \"ein\",        # ع\n",
    "    \"fe\",         # ف\n",
    "    \"ghaf\",       # ق\n",
    "    \"kaf\",        # ک\n",
    "    \"gaf\",        # گ\n",
    "    \"lam\",        # ل\n",
    "    \"mim\",        # م\n",
    "    \"noon\",       # ن\n",
    "    \"vav\",        # و\n",
    "    \"he\",         # ه\n",
    "    \"ye\",         # ی\n",
    "    \"ze\",         # ز\n",
    "    \"taxi\",       # تاکسی\n",
    "    \"disabled\",   # معلولین\n",
    "    \"police\",     # پلیس\n",
    "    \"D\",          # Diplomatic\n",
    "    \"S\",          # Service\n",
    "    \"protocol\",   # تشریفات\n",
    "]\n",
    "\n",
    "# Mapping Persian labels to class indices\n",
    "LABEL_MAP = {\n",
    "    \"کل ناحیه پلاک\": CLASS_NAMES.index(\"plate\"),\n",
    "    # Numbers\n",
    "    \"0\": CLASS_NAMES.index(\"0\"), \"1\": CLASS_NAMES.index(\"1\"), \n",
    "    \"2\": CLASS_NAMES.index(\"2\"), \"3\": CLASS_NAMES.index(\"3\"),\n",
    "    \"4\": CLASS_NAMES.index(\"4\"), \"5\": CLASS_NAMES.index(\"5\"), \n",
    "    \"6\": CLASS_NAMES.index(\"6\"), \"7\": CLASS_NAMES.index(\"7\"),\n",
    "    \"8\": CLASS_NAMES.index(\"8\"), \"9\": CLASS_NAMES.index(\"9\"),\n",
    "    # Persian alphabets\n",
    "    \"الف\": CLASS_NAMES.index(\"alef\"),\n",
    "    \"ب\": CLASS_NAMES.index(\"be\"),\n",
    "    \"پ\": CLASS_NAMES.index(\"pe\"),\n",
    "    \"ت\": CLASS_NAMES.index(\"te\"),\n",
    "    \"ث\": CLASS_NAMES.index(\"se\"),\n",
    "    \"ج\": CLASS_NAMES.index(\"jim\"),\n",
    "    \"د\": CLASS_NAMES.index(\"dal\"),\n",
    "    \"س\": CLASS_NAMES.index(\"sin\"),\n",
    "    \"ش\": CLASS_NAMES.index(\"shin\"),\n",
    "    \"ص\": CLASS_NAMES.index(\"sad\"),\n",
    "    \"ط\": CLASS_NAMES.index(\"ta\"),\n",
    "    \"ظ\": CLASS_NAMES.index(\"za\"),\n",
    "    \"ع\": CLASS_NAMES.index(\"ein\"),\n",
    "    \"ف\": CLASS_NAMES.index(\"fe\"),\n",
    "    \"ق\": CLASS_NAMES.index(\"ghaf\"),\n",
    "    \"ک\": CLASS_NAMES.index(\"kaf\"),\n",
    "    \"گ\": CLASS_NAMES.index(\"gaf\"),\n",
    "    \"ل\": CLASS_NAMES.index(\"lam\"),\n",
    "    \"م\": CLASS_NAMES.index(\"mim\"),\n",
    "    \"ن\": CLASS_NAMES.index(\"noon\"),\n",
    "    \"و\": CLASS_NAMES.index(\"vav\"),\n",
    "    \"ه‌\": CLASS_NAMES.index(\"he\"),\n",
    "    \"ه\": CLASS_NAMES.index(\"he\"),\n",
    "    \"ی\": CLASS_NAMES.index(\"ye\"),\n",
    "    \"ز\": CLASS_NAMES.index(\"ze\"),\n",
    "    # Special symbols\n",
    "    \"ژ (معلولین و جانبازان)\": CLASS_NAMES.index(\"disabled\"),\n",
    "    \"تشریفات\": CLASS_NAMES.index(\"protocol\"),\n",
    "    \"S\": CLASS_NAMES.index(\"S\"),\n",
    "    \"D\": CLASS_NAMES.index(\"D\"),\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def convert_box(size, box):\n",
    "    \"\"\"\n",
    "    Convert bounding box from (xmin, ymin, xmax, ymax) to YOLO format.\n",
    "    YOLO format: (x_center, y_center, width, height) - all normalized to [0,1]\n",
    "    \n",
    "    Args:\n",
    "        size: tuple (image_width, image_height)\n",
    "        box: tuple (xmin, xmax, ymin, ymax)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (x_center, y_center, width, height) normalized\n",
    "    \"\"\"\n",
    "    dw = 1.0 / size[0]\n",
    "    dh = 1.0 / size[1]\n",
    "    \n",
    "    x_center = (box[0] + box[1]) / 2.0\n",
    "    y_center = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    x_center = x_center * dw\n",
    "    w = w * dw\n",
    "    y_center = y_center * dh\n",
    "    h = h * dh\n",
    "    \n",
    "    return (x_center, y_center, w, h)\n",
    "\n",
    "\n",
    "def process_dataset():\n",
    "    \"\"\"\n",
    "    Main function to process the entire dataset:\n",
    "    1. Create YOLO directory structure\n",
    "    2. Convert XML annotations to YOLO format\n",
    "    3. Copy images and create label files\n",
    "    4. Generate data.yaml config file\n",
    "    \"\"\"\n",
    "    print(\"Starting dataset conversion...\")\n",
    "    \n",
    "    # Create output directory structure\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        os.makedirs(os.path.join(OUTPUT_DIR, \"images\", split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(OUTPUT_DIR, \"labels\", split), exist_ok=True)\n",
    "\n",
    "    # Process each source directory\n",
    "    for source_dir in SOURCE_DIRS:\n",
    "        if not os.path.exists(source_dir):\n",
    "            print(f\"Warning: Directory '{source_dir}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Process each data split (train/val/test)\n",
    "        for original_split, yolo_split in SPLIT_MAPPING.items():\n",
    "            current_path = os.path.join(source_dir, original_split)\n",
    "            \n",
    "            if not os.path.exists(current_path):\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing: {current_path} -> {yolo_split}\")\n",
    "            \n",
    "            files = os.listdir(current_path)\n",
    "            # Get unique filenames (without extension) for pairing XML and JPG\n",
    "            filenames = set([os.path.splitext(f)[0] for f in files if f.endswith('.xml')])\n",
    "\n",
    "            for name in filenames:\n",
    "                jpg_file = os.path.join(current_path, name + \".jpg\")\n",
    "                xml_file = os.path.join(current_path, name + \".xml\")\n",
    "                \n",
    "                # Verify image exists\n",
    "                if not os.path.exists(jpg_file):\n",
    "                    print(f\"Missing image for {xml_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Read image to get actual dimensions\n",
    "                img = cv2.imread(jpg_file)\n",
    "                if img is None:\n",
    "                    print(f\"Corrupt image: {jpg_file}\")\n",
    "                    continue\n",
    "                \n",
    "                h, w, _ = img.shape\n",
    "                \n",
    "                # Parse XML annotation\n",
    "                try:\n",
    "                    tree = ET.parse(xml_file)\n",
    "                    root = tree.getroot()\n",
    "                except ET.ParseError:\n",
    "                    print(f\"XML Parse Error: {xml_file}\")\n",
    "                    continue\n",
    "\n",
    "                yolo_lines = []\n",
    "                \n",
    "                # Extract each object annotation\n",
    "                for obj in root.findall('object'):\n",
    "                    class_name_raw = obj.find('name').text.strip()\n",
    "                    \n",
    "                    # Map Persian label to class ID\n",
    "                    if class_name_raw in LABEL_MAP:\n",
    "                        class_id = LABEL_MAP[class_name_raw]\n",
    "                    else:\n",
    "                        print(f\"Warning: Unknown label '{class_name_raw}' in {xml_file}. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract bounding box coordinates\n",
    "                    xml_box = obj.find('bndbox')\n",
    "                    b = (\n",
    "                        float(xml_box.find('xmin').text),\n",
    "                        float(xml_box.find('xmax').text),\n",
    "                        float(xml_box.find('ymin').text),\n",
    "                        float(xml_box.find('ymax').text)\n",
    "                    )\n",
    "                    \n",
    "                    # Convert to YOLO format\n",
    "                    bb = convert_box((w, h), b)\n",
    "                    yolo_lines.append(f\"{class_id} {bb[0]:.6f} {bb[1]:.6f} {bb[2]:.6f} {bb[3]:.6f}\")\n",
    "\n",
    "                # Save only if valid objects were found\n",
    "                if yolo_lines:\n",
    "                    # Create unique filename to avoid conflicts\n",
    "                    unique_name = f\"{source_dir}_{original_split}_{name}\"\n",
    "                    \n",
    "                    target_img_path = os.path.join(OUTPUT_DIR, \"images\", yolo_split, unique_name + \".jpg\")\n",
    "                    target_lbl_path = os.path.join(OUTPUT_DIR, \"labels\", yolo_split, unique_name + \".txt\")\n",
    "                    \n",
    "                    # Copy image\n",
    "                    shutil.copy(jpg_file, target_img_path)\n",
    "                    \n",
    "                    # Write label file\n",
    "                    with open(target_lbl_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write('\\n'.join(yolo_lines))\n",
    "\n",
    "    print(\"Dataset conversion completed successfully.\")\n",
    "    create_yaml()\n",
    "\n",
    "\n",
    "def create_yaml():\n",
    "    \"\"\"\n",
    "    Generate the data.yaml configuration file required by YOLO training.\n",
    "    Contains paths, number of classes, and class names.\n",
    "    \"\"\"\n",
    "    yaml_content = {\n",
    "        'path': os.path.abspath(OUTPUT_DIR),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'nc': len(CLASS_NAMES),\n",
    "        'names': CLASS_NAMES\n",
    "    }\n",
    "    \n",
    "    yaml_path = os.path.join(OUTPUT_DIR, \"data.yaml\")\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_content, f, sort_keys=False)\n",
    "    \n",
    "    print(f\"Created config file at: {yaml_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## بخش دوم: افزایش داده (Data Augmentation)\n",
    "\n",
    "### مشکل عدم تعادل کلاس‌ها\n",
    "\n",
    "یکی از چالش‌های اصلی در آموزش مدل، عدم تعادل شدید بین کلاس‌های مختلف بود. به طور مثال:\n",
    "- کلاس‌های حروف رایج (الف، ب، د): 70,000 نمونه\n",
    "- کلاس‌های خاص (پ، ث، ش، ظ، گ، ک، معلولین، تشریفات، D، S): کمتر از 3,000 نمونه\n",
    "\n",
    "این عدم تعادل باعث می‌شود مدل در تشخیص کلاس‌های کم‌نمونه عملکرد ضعیفی داشته باشد.\n",
    "\n",
    "### راه‌حل: افزایش داده هدفمند\n",
    "\n",
    "برای حل این مشکل، یک سیستم افزایش داده هدفمند طراحی شد که:\n",
    "\n",
    "1. **هدف‌گیری هوشمند**: فقط کلاس‌هایی که زیر 5,000 نمونه دارند را افزایش می‌دهد\n",
    "2. **حفظ یکپارچگی**: تصاویر را به همراه تمام bounding box های آن‌ها augment می‌کند\n",
    "3. **تنوع بالا**: از ترکیب چندین تکنیک augmentation استفاده می‌کند\n",
    "4. **جلوگیری از flip**: چون پلاک‌های ایران قابل قرینه شدن نیستند، از horizontal/vertical flip استفاده نمی‌شود\n",
    "\n",
    "### تکنیک‌های Augmentation استفاده شده\n",
    "\n",
    "- **چرخش (Rotation)**: تا 25 درجه (با احتمال 80%)\n",
    "- **تغییر روشنایی و کنتراست**: ±20% (احتمال 50%)\n",
    "- **تغییر رنگ (Hue/Saturation)**: تغییرات جزئی (احتمال 40%)\n",
    "- **افزودن نویز Gaussian**: (احتمال 30%)\n",
    "- **Motion Blur**: برای شبیه‌سازی حرکت (احتمال 20%)\n",
    "\n",
    "### نتیجه\n",
    "\n",
    "پس از اجرای این فرآیند، دیتاست از 76,000 به حدود 105,000 تصویر افزایش یافت و تمام کلاس‌های هدف به حداقل 5,000 نمونه رسیدند.\n",
    "\n",
    "## Augmentation\n",
    "aug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "DATASET_DIR = \"yolo_dataset\"\n",
    "TARGET_COUNT = 5000  # Minimum required instances per target class\n",
    "\n",
    "# All 41 classes defined in the model\n",
    "CLASS_NAMES = [\n",
    "    \"plate\",      \n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \n",
    "    \"alef\", \"be\", \"pe\", \"te\", \"se\", \"jim\", \"dal\", \"ze\", \"sin\", \n",
    "    \"shin\", \"sad\", \"ta\", \"za\", \"ein\", \"fe\", \"ghaf\", \"kaf\", \n",
    "    \"gaf\", \"lam\", \"mim\", \"noon\", \"vav\", \"he\", \"ye\", \"zhe\", \n",
    "    \"disabled\", \"protocol\", \"S\", \"D\",\n",
    "]\n",
    "\n",
    "# Classes that need augmentation (underrepresented in dataset)\n",
    "TARGET_CLASSES_NAMES = [\n",
    "    \"alef\", \"pe\", \"se\", \"shin\", \"za\", \"kaf\", \"gaf\", \n",
    "    \"disabled\", \"protocol\", \"S\", \"D\"\n",
    "]\n",
    "\n",
    "# Convert class names to indices\n",
    "TARGET_CLASS_IDS = [CLASS_NAMES.index(name) for name in TARGET_CLASSES_NAMES if name in CLASS_NAMES]\n",
    "\n",
    "# ==========================================\n",
    "# AUGMENTATION PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "\"\"\"\n",
    "Augmentation pipeline designed specifically for license plates:\n",
    "- NO horizontal/vertical flips (Iranian plates are not symmetric)\n",
    "- Moderate rotation (up to 25 degrees)\n",
    "- Color and brightness variations to simulate different lighting\n",
    "- Noise and blur to simulate real-world conditions\n",
    "\"\"\"\n",
    "aug_pipeline = A.Compose([\n",
    "    A.Rotate(limit=25, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.8),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.4),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "    A.MotionBlur(blur_limit=3, p=0.2),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "# ==========================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def read_yolo_label(txt_path):\n",
    "    \"\"\"\n",
    "    Read YOLO format label file and extract bounding boxes and class IDs.\n",
    "    \n",
    "    Args:\n",
    "        txt_path: Path to .txt label file\n",
    "    \n",
    "    Returns:\n",
    "        boxes: List of bounding boxes in YOLO format [x_center, y_center, width, height]\n",
    "        class_labels: List of corresponding class IDs\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    class_labels = []\n",
    "    \n",
    "    if not os.path.exists(txt_path):\n",
    "        return boxes, class_labels\n",
    "        \n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 5:\n",
    "            cls_id = int(parts[0])\n",
    "            bbox = [float(x) for x in parts[1:]]  # x_center, y_center, width, height\n",
    "            \n",
    "            boxes.append(bbox)\n",
    "            class_labels.append(cls_id)\n",
    "            \n",
    "    return boxes, class_labels\n",
    "\n",
    "\n",
    "def save_yolo_label(txt_path, boxes, class_labels):\n",
    "    \"\"\"\n",
    "    Save bounding boxes and class labels to YOLO format text file.\n",
    "    Ensures all values are properly clipped to [0,1] range.\n",
    "    \n",
    "    Args:\n",
    "        txt_path: Output path for label file\n",
    "        boxes: List of bounding boxes\n",
    "        class_labels: List of class IDs\n",
    "    \"\"\"\n",
    "    with open(txt_path, 'w') as f:\n",
    "        for bbox, cls_id in zip(boxes, class_labels):\n",
    "            # Clip values to valid range [0,1]\n",
    "            xc, yc, w, h = bbox\n",
    "            xc = max(0.0, min(1.0, xc))\n",
    "            yc = max(0.0, min(1.0, yc))\n",
    "            w = max(0.0, min(1.0, w))\n",
    "            h = max(0.0, min(1.0, h))\n",
    "            \n",
    "            f.write(f\"{cls_id} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# MAIN BALANCING PROCESS\n",
    "# ==========================================\n",
    "\n",
    "def balance_dataset():\n",
    "    \"\"\"\n",
    "    Main function to balance the dataset by augmenting underrepresented classes.\n",
    "    \n",
    "    Process:\n",
    "    1. Index all images containing target classes\n",
    "    2. For each class below TARGET_COUNT:\n",
    "       - Randomly select images containing that class\n",
    "       - Apply augmentation pipeline\n",
    "       - Save augmented images and labels\n",
    "    3. Continue until all classes reach TARGET_COUNT\n",
    "    \"\"\"\n",
    "    print(f\"Target classes to augment: {TARGET_CLASSES_NAMES}\")\n",
    "    print(f\"Target count per class: {TARGET_COUNT}\")\n",
    "\n",
    "    # Index dataset - map each class to images containing it\n",
    "    splits = ['train', 'val', 'test']\n",
    "    all_files_map = {cid: [] for cid in TARGET_CLASS_IDS}\n",
    "    \n",
    "    print(\"Indexing dataset...\")\n",
    "    for split in splits:\n",
    "        img_dir = os.path.join(DATASET_DIR, \"images\", split)\n",
    "        lbl_dir = os.path.join(DATASET_DIR, \"labels\", split)\n",
    "        \n",
    "        txt_files = glob.glob(os.path.join(lbl_dir, \"*.txt\"))\n",
    "        \n",
    "        for txt_file in tqdm(txt_files, desc=f\"Scanning {split}\"):\n",
    "            _, class_ids_in_file = read_yolo_label(txt_file)\n",
    "            \n",
    "            # Find corresponding image\n",
    "            basename = os.path.basename(txt_file).replace('.txt', '.jpg')\n",
    "            img_path = os.path.join(img_dir, basename)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "                \n",
    "            # Map classes to this image\n",
    "            unique_classes = set(class_ids_in_file)\n",
    "            for cid in unique_classes:\n",
    "                if cid in TARGET_CLASS_IDS:\n",
    "                    all_files_map[cid].append({\n",
    "                        'img_path': img_path,\n",
    "                        'txt_path': txt_file,\n",
    "                        'split': split\n",
    "                    })\n",
    "\n",
    "    # Augment each class as needed\n",
    "    print(\"\\n--- Augmentation Status ---\")\n",
    "    \n",
    "    for cid in TARGET_CLASS_IDS:\n",
    "        current_count = len(all_files_map[cid])\n",
    "        class_name = CLASS_NAMES[cid]\n",
    "        \n",
    "        if current_count >= TARGET_COUNT:\n",
    "            print(f\"Class '{class_name}' (ID {cid}): OK ({current_count} instances)\")\n",
    "            continue\n",
    "            \n",
    "        needed = TARGET_COUNT - current_count\n",
    "        print(f\"Class '{class_name}' (ID {cid}): Needs {needed} more (Current: {current_count})\")\n",
    "        \n",
    "        if current_count == 0:\n",
    "            print(f\"  WARNING: No instances found for '{class_name}'. Cannot augment.\")\n",
    "            continue\n",
    "\n",
    "        # Generate augmented samples\n",
    "        generated_count = 0\n",
    "        pbar = tqdm(total=needed, desc=f\"Augmenting {class_name}\")\n",
    "        \n",
    "        while generated_count < needed:\n",
    "            # Randomly select source image containing this class\n",
    "            source_data = random.choice(all_files_map[cid])\n",
    "            \n",
    "            src_img_path = source_data['img_path']\n",
    "            src_txt_path = source_data['txt_path']\n",
    "            \n",
    "            # Load image\n",
    "            image = cv2.imread(src_img_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load labels\n",
    "            bboxes, labels = read_yolo_label(src_txt_path)\n",
    "            \n",
    "            # Apply augmentation\n",
    "            try:\n",
    "                transformed = aug_pipeline(image=image, bboxes=bboxes, class_labels=labels)\n",
    "                aug_img = transformed['image']\n",
    "                aug_bboxes = transformed['bboxes']\n",
    "                aug_labels = transformed['class_labels']\n",
    "                \n",
    "                # Skip if augmentation removed all boxes\n",
    "                if len(aug_bboxes) == 0 and len(bboxes) > 0:\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Skip failed augmentations\n",
    "                continue\n",
    "                \n",
    "            # Save augmented data\n",
    "            img_dir = os.path.dirname(src_img_path)\n",
    "            lbl_dir = os.path.dirname(src_txt_path)\n",
    "            \n",
    "            # Create unique filename\n",
    "            base_name = os.path.splitext(os.path.basename(src_img_path))[0]\n",
    "            new_name = f\"{base_name}_aug_{generated_count}_{random.randint(100,999)}\"\n",
    "            \n",
    "            target_img_path = os.path.join(img_dir, new_name + \".jpg\")\n",
    "            target_txt_path = os.path.join(lbl_dir, new_name + \".txt\")\n",
    "            \n",
    "            # Write image (convert back to BGR for OpenCV)\n",
    "            cv2.imwrite(target_img_path, cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # Write label\n",
    "            save_yolo_label(target_txt_path, aug_bboxes, aug_labels)\n",
    "            \n",
    "            generated_count += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "        pbar.close()\n",
    "\n",
    "    print(\"\\nData balancing complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    balance_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## بخش سوم: آموزش مدل\n",
    "\n",
    "### انتخاب مدل پایه\n",
    "\n",
    "برای این پروژه از **YOLOv8n** (نسخه Nano) استفاده شد. دلایل این انتخاب:\n",
    "\n",
    "1. **سبک‌وزن بودن**: با حدود 3.2 میلیون پارامتر، برای سخت‌افزار محدود مناسب است\n",
    "2. **سرعت بالا**: قابلیت پردازش real-time حتی بر روی CPU\n",
    "3. **دقت قابل قبول**: با وجود سبک بودن، دقت مناسبی برای detection دارد\n",
    "4. **پشتیبانی عالی**: کتابخانه Ultralytics به‌روز و کامل است\n",
    "\n",
    "### تنظیمات آموزش\n",
    "\n",
    "**پارامترهای کلیدی:**\n",
    "- **Epochs**: 20 (با early stopping patience=10)\n",
    "- **Batch size**: 64\n",
    "- **Image size**: 640×640\n",
    "- **Device**: GPU (CUDA)\n",
    "\n",
    "**غیرفعال‌سازی augmentation‌های نامناسب:**\n",
    "\n",
    "YOLO به‌صورت پیش‌فرض از flip و mixup استفاده می‌کند که برای پلاک ایرانی مناسب نیست:\n",
    "- `fliplr: 0.0` - چون پلاک قابل قرینه شدن نیست\n",
    "- `flipud: 0.0` - مشابه بالا\n",
    "- `mixup: 0.0` - ترکیب دو تصویر منطقی نیست\n",
    "- `copy_paste: 0.0` - برای پلاک کاربردی نیست\n",
    "\n",
    "### معیارهای ارزیابی\n",
    "\n",
    "مدل بر اساس معیارهای زیر ارزیابی می‌شود:\n",
    "- **Precision**: نسبت تشخیص‌های درست به کل تشخیص‌ها\n",
    "- **Recall**: نسبت تشخیص‌های درست به کل موارد واقعی\n",
    "- **mAP@50**: میانگین دقت در IoU threshold 0.5\n",
    "- **mAP@50-95**: میانگین دقت در range مختلف IoU (معیار جامع‌تر)\n",
    "\n",
    "## Train\n",
    "train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Train YOLOv8n model on Iranian license plate dataset.\n",
    "    \n",
    "    Configuration:\n",
    "    - Base model: YOLOv8n (nano - lightweight for embedded systems)\n",
    "    - Custom augmentations disabled (flips not suitable for plates)\n",
    "    - Early stopping enabled (patience=10)\n",
    "    - GPU training (device=0)\n",
    "    \"\"\"\n",
    "    # Load pretrained YOLOv8n model\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    # Path to dataset configuration\n",
    "    yaml_path = os.path.join(\"yolo_dataset\", \"data.yaml\")\n",
    "\n",
    "    # Disable unsuitable augmentations for license plates\n",
    "    custom_augs = {\n",
    "        'fliplr': 0.0,      # No horizontal flip (plates are not symmetric)\n",
    "        'flipud': 0.0,      # No vertical flip\n",
    "        'mixup': 0.0,       # No mixup (doesn't make sense for plates)\n",
    "        'copy_paste': 0.0,  # No copy-paste augmentation\n",
    "    }\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=20,\n",
    "        imgsz=640,\n",
    "        batch=64,\n",
    "        name='iran_lpr_model',\n",
    "        device=0,  # Use first GPU\n",
    "        patience=10,  # Early stopping patience\n",
    "        verbose=True,\n",
    "        **custom_augs\n",
    "    )\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    \n",
    "    # Validate the trained model\n",
    "    print(\"\\nValidating model...\")\n",
    "    metrics = model.val()\n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"mAP@50-95: {metrics.box.map:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## بخش چهارم: تست مدل\n",
    "\n",
    "### فرآیند تست\n",
    "\n",
    "برای ارزیابی عملکرد مدل در شرایط واقعی، یک اسکریپت تست طراحی شد که:\n",
    "\n",
    "1. تصاویر تست را از یک پوشه می‌خواند\n",
    "2. مدل را روی هر تصویر اجرا می‌کند\n",
    "3. پلاک‌ها را شناسایی کرده و کاراکترهای داخل آن‌ها را استخراج می‌کند\n",
    "4. کاراکترها را بر اساس موقعیت افقی مرتب می‌کند\n",
    "5. متن پلاک را به فرمت استاندارد ایرانی نمایش می‌دهد\n",
    "6. تصویر خروجی با bounding box ها و متن پلاک را ذخیره می‌کند\n",
    "\n",
    "### فرمت نمایش پلاک\n",
    "\n",
    "پلاک ایرانی به فرمت زیر نمایش داده می‌شود:\n",
    "```\n",
    "[دو رقم] [حرف] [سه رقم سریال] | [دو رقم شهر]\n",
    "مثال: 12 | 345 ب 67\n",
    "```\n",
    "\n",
    "### توابع کمکی\n",
    "\n",
    "- **is_inside()**: بررسی می‌کند آیا مرکز یک bounding box داخل bounding box دیگری است\n",
    "- **format_plate_text()**: کاراکترها را به فرمت استاندارد پلاک تبدیل می‌کند\n",
    "\n",
    "## Test\n",
    "test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "MODEL_PATH = \"model/best.pt\"  # Path to trained model weights\n",
    "INPUT_PATH = \"test_model/\"  # Folder containing test images\n",
    "OUTPUT_FOLDER = \"output_results\"  # Output folder for annotated images\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"plate\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \n",
    "    \"alef\", \"be\", \"pe\", \"te\", \"se\", \"jim\", \"dal\", \"ze\", \"sin\", \n",
    "    \"shin\", \"sad\", \"ta\", \"za\", \"ein\", \"fe\", \"ghaf\", \"kaf\", \n",
    "    \"gaf\", \"lam\", \"mim\", \"noon\", \"vav\", \"he\", \"ye\", \"zhe\", \n",
    "    \"disabled\", \"protocol\", \"S\", \"D\"\n",
    "]\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "# ==========================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def is_inside(box_inner, box_outer):\n",
    "    \"\"\"\n",
    "    Check if the center of box_inner is inside box_outer.\n",
    "    Used to determine which characters belong to which plate.\n",
    "    \n",
    "    Args:\n",
    "        box_inner: [x1, y1, x2, y2] of character box\n",
    "        box_outer: [x1, y1, x2, y2] of plate box\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if center of inner box is inside outer box\n",
    "    \"\"\"\n",
    "    c_x = (box_inner[0] + box_inner[2]) / 2\n",
    "    c_y = (box_inner[1] + box_inner[3]) / 2\n",
    "    return (box_outer[0] < c_x < box_outer[2]) and (box_outer[1] < c_y < box_outer[3])\n",
    "\n",
    "\n",
    "def format_plate_text(chars_list):\n",
    "    \"\"\"\n",
    "    Format detected characters into Iranian plate standard format.\n",
    "    Format: [2 digits] [letter] [3 digits serial] | [2 digits city code]\n",
    "    Example: 67 ب 345 | 12\n",
    "    \n",
    "    Args:\n",
    "        chars_list: List of character classes sorted left to right\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted plate text\n",
    "    \"\"\"\n",
    "    if len(chars_list) < 8:\n",
    "        # If less than 8 characters, return simple concatenation\n",
    "        return \" \".join(chars_list)\n",
    "    \n",
    "    # Standard Iranian plate format:\n",
    "    # [0,1]: First two digits (left side)\n",
    "    # [2]: Letter\n",
    "    # [3,4,5]: Three middle digits\n",
    "    # [6,7]: City code (right side)\n",
    "    try:\n",
    "        part1 = \"\".join(chars_list[0:2])    # First 2 digits\n",
    "        part2 = \"\".join(chars_list[3:6])    # Middle 3 digits\n",
    "        letter = chars_list[2]              # Letter\n",
    "        part3 = \"\".join(chars_list[6:8])    # Last 2 digits (city code)\n",
    "        \n",
    "        return f\"{part1} {letter} {part2} | {part3}\"\n",
    "    except:\n",
    "        return \" \".join(chars_list)\n",
    "\n",
    "# ==========================================\n",
    "# IMAGE PROCESSING\n",
    "# ==========================================\n",
    "\n",
    "def process_images():\n",
    "    \"\"\"\n",
    "    Process all test images:\n",
    "    1. Load images from INPUT_PATH\n",
    "    2. Run YOLO inference\n",
    "    3. Extract plates and characters\n",
    "    4. Format and display plate text\n",
    "    5. Save annotated images to OUTPUT_FOLDER\n",
    "    \"\"\"\n",
    "    # Load trained model\n",
    "    print(f\"Loading model: {MODEL_PATH}\")\n",
    "    model = YOLO(MODEL_PATH)\n",
    "    \n",
    "    # Get list of images to process\n",
    "    if os.path.isfile(INPUT_PATH):\n",
    "        image_files = [INPUT_PATH]\n",
    "    else:\n",
    "        image_files = [os.path.join(INPUT_PATH, f) for f in os.listdir(INPUT_PATH) \n",
    "                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    if not image_files:\n",
    "        print(\"No images found in the specified path!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {len(image_files)} images...\\n\")\n",
    "\n",
    "    for img_path in image_files:\n",
    "        # Read image\n",
    "        frame = cv2.imread(img_path)\n",
    "        if frame is None:\n",
    "            print(f\"Failed to read: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Run YOLO inference (CPU mode by default)\n",
    "        results = model(frame, verbose=False)[0]\n",
    "        \n",
    "        # Extract detections\n",
    "        detections = []\n",
    "        for box in results.boxes.data.tolist():\n",
    "            x1, y1, x2, y2, score, class_id = box\n",
    "            detections.append({\n",
    "                'box': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                'class': CLASS_NAMES[int(class_id)],\n",
    "                'score': score\n",
    "            })\n",
    "\n",
    "        # Separate plates and characters\n",
    "        plates = [d for d in detections if d['class'] == 'plate']\n",
    "        characters = [d for d in detections if d['class'] != 'plate']\n",
    "\n",
    "        # Process each detected plate\n",
    "        for plate in plates:\n",
    "            px1, py1, px2, py2 = plate['box']\n",
    "            \n",
    "            # Find characters inside this plate\n",
    "            plate_chars = [c for c in characters if is_inside(c['box'], plate['box'])]\n",
    "            \n",
    "            # Sort characters left to right\n",
    "            plate_chars.sort(key=lambda x: x['box'][0])\n",
    "            \n",
    "            # Extract text\n",
    "            text_list = [c['class'] for c in plate_chars]\n",
    "            plate_text = format_plate_text(text_list)\n",
    "            \n",
    "            # Draw plate bounding box\n",
    "            cv2.rectangle(frame, (px1, py1), (px2, py2), (0, 255, 0), 3)\n",
    "            \n",
    "            # Draw text label above plate\n",
    "            label = f\"{plate_text}\"\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "            \n",
    "            # Draw background for text\n",
    "            cv2.rectangle(frame, (px1, py1 - h - 15), (px1 + w, py1), (0, 165, 255), -1)\n",
    "            \n",
    "            # Draw text\n",
    "            cv2.putText(frame, label, (px1, py1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # Save annotated image\n",
    "        save_path = os.path.join(OUTPUT_FOLDER, os.path.basename(img_path))\n",
    "        cv2.imwrite(save_path, frame)\n",
    "        print(f\"Processed: {os.path.basename(img_path)} -> Saved to {OUTPUT_FOLDER}\")\n",
    "\n",
    "    print(\"\\nAll images processed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## بخش پنجم: پیاده‌سازی روی Raspberry Pi\n",
    "\n",
    "### چالش‌های پیاده‌سازی\n",
    "\n",
    "پیاده‌سازی مدل روی Raspberry Pi 5 با چالش‌های زیر همراه بود:\n",
    "\n",
    "#### 1. محدودیت‌های سخت‌افزاری\n",
    "- **RAM**: 2GB (محدود برای مدل‌های deep learning)\n",
    "- **CPU**: Quad-core ARM Cortex-A76 (بدون GPU قوی)\n",
    "- **حافظه**: نبود SD Card با ظرفیت کافی\n",
    "\n",
    "**راه‌حل**: بوت از USB Flash با ظرفیت بالاتر پس از آپدیت EEPROM\n",
    "\n",
    "#### 2. مشکلات دوربین\n",
    "دوربین RPi Camera Module از طریق پورت MIPI CSI متصل می‌شود و با کتابخانه‌های استاندارد OpenCV کار نمی‌کند.\n",
    "\n",
    "**راه‌حل**: استفاده از کتابخانه PiCamZero که interface ساده‌ای برای دسترسی به libcamera فراهم می‌کند\n",
    "\n",
    "#### 3. بهینه‌سازی مدل\n",
    "مدل PyTorch (.pt) برای CPU معمولی بهینه نیست.\n",
    "\n",
    "**راه‌حل**: تبدیل وزن‌های مدل به فرمت NCNN که برای ARM CPUs بهینه‌سازی شده است:\n",
    "```bash\n",
    "yolo export model=best.pt format=ncnn\n",
    "```\n",
    "\n",
    "### معماری سیستم نهایی\n",
    "\n",
    "```\n",
    "RPi Camera Module\n",
    "       ↓\n",
    "MIPI CSI Interface\n",
    "       ↓\n",
    "PiCamZero Library\n",
    "       ↓\n",
    "NumPy Array (640×480)\n",
    "       ↓\n",
    "YOLOv8n-NCNN Model\n",
    "       ↓\n",
    "Detection Results\n",
    "       ↓\n",
    "Console Output (FPS: ~1.5)\n",
    "```\n",
    "\n",
    "### عملکرد نهایی\n",
    "\n",
    "- **FPS**: حدود 1.5 فریم در ثانیه\n",
    "- **دقت**: مشابه مدل اصلی\n",
    "- **تأخیر**: حدود 650-700 میلی‌ثانیه برای هر فریم\n",
    "\n",
    "این سرعت برای یک سیستم پارکینگ که ماشین‌ها با سرعت کم حرکت می‌کنند، کاملاً قابل قبول است.\n",
    "\n",
    "## Raspberry Pi\n",
    "rpi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "from picamzero import Camera  # Raspberry Pi camera library\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "MODEL_PATH = \"best_ncnn_model_640\"  # NCNN optimized model for ARM CPU\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"plate\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \n",
    "    \"alef\", \"be\", \"pe\", \"te\", \"se\", \"jim\", \"dal\", \"ze\", \"sin\", \n",
    "    \"shin\", \"sad\", \"ta\", \"za\", \"ein\", \"fe\", \"ghaf\", \"kaf\", \n",
    "    \"gaf\", \"lam\", \"mim\", \"noon\", \"vav\", \"he\", \"ye\", \"zhe\", \n",
    "    \"disabled\", \"protocol\", \"S\", \"D\",\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def is_inside(box_inner, box_outer):\n",
    "    \"\"\"\n",
    "    Check if center of inner box is inside outer box.\n",
    "    Used to associate characters with their parent plate.\n",
    "    \"\"\"\n",
    "    c_x = (box_inner[0] + box_inner[2]) / 2\n",
    "    c_y = (box_inner[1] + box_inner[3]) / 2\n",
    "    return (box_outer[0] < c_x < box_outer[2]) and (box_outer[1] < c_y < box_outer[3])\n",
    "\n",
    "\n",
    "def format_plate_text(chars_list):\n",
    "    \"\"\"\n",
    "    Simple concatenation of detected characters.\n",
    "    Returns a readable string of the plate.\n",
    "    \"\"\"\n",
    "    return \"\".join(chars_list)\n",
    "\n",
    "# ==========================================\n",
    "# MAIN PROCESS\n",
    "# ==========================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main loop for real-time license plate recognition on Raspberry Pi.\n",
    "    \n",
    "    Process:\n",
    "    1. Initialize RPi Camera using PiCamZero\n",
    "    2. Load NCNN-optimized YOLO model\n",
    "    3. Continuous frame capture and inference\n",
    "    4. Extract and display detected plates\n",
    "    5. Calculate and display FPS\n",
    "    \"\"\"\n",
    "    print(\"--- License Plate Recognition with PiCamZero ---\")\n",
    "    \n",
    "    # Initialize Raspberry Pi Camera\n",
    "    try:\n",
    "        cam = Camera()\n",
    "        # PiCamZero handles libcamera configuration automatically\n",
    "        print(\"Camera initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not initialize camera. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load NCNN-optimized model\n",
    "    print(f\"Loading model: {MODEL_PATH}\")\n",
    "    try:\n",
    "        model = YOLO(MODEL_PATH, task='detect')\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Model load error: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Starting Detection Loop. Press Ctrl+C to stop.\\n\")\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Capture frame from camera\n",
    "            # capture_array() returns numpy array compatible with OpenCV\n",
    "            frame = cam.capture_array()\n",
    "\n",
    "            # Calculate FPS\n",
    "            curr_time = time.time()\n",
    "            fps = 1 / (curr_time - prev_time)\n",
    "            prev_time = curr_time\n",
    "\n",
    "            # Run YOLO inference\n",
    "            # conf=0.5: minimum confidence threshold\n",
    "            # verbose=False: suppress detailed output\n",
    "            results = model(frame, verbose=False, conf=0.5)[0]\n",
    "\n",
    "            # Extract detections\n",
    "            detections = []\n",
    "            for box in results.boxes.data.tolist():\n",
    "                x1, y1, x2, y2, score, class_id = box\n",
    "                detections.append({\n",
    "                    'box': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                    'class': CLASS_NAMES[int(class_id)]\n",
    "                })\n",
    "\n",
    "            # Separate plates and characters\n",
    "            plates = [d for d in detections if d['class'] == 'plate']\n",
    "            characters = [d for d in detections if d['class'] != 'plate']\n",
    "\n",
    "            # Process each detected plate\n",
    "            if plates:\n",
    "                for plate in plates:\n",
    "                    # Find characters belonging to this plate\n",
    "                    plate_chars = [c for c in characters if is_inside(c['box'], plate['box'])]\n",
    "                    \n",
    "                    # Sort characters left to right\n",
    "                    plate_chars.sort(key=lambda x: x['box'][0])\n",
    "                    \n",
    "                    # Format plate text\n",
    "                    text = format_plate_text([c['class'] for c in plate_chars])\n",
    "                    \n",
    "                    if text:\n",
    "                        # Print detection with timestamp and FPS\n",
    "                        print(f\"[{time.strftime('%H:%M:%S')}] PLATE DETECTED: {text} (FPS: {fps:.1f})\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping script...\")\n",
    "    finally:\n",
    "        # PiCamZero automatically releases camera on object destruction\n",
    "        print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## نتیجه‌گیری\n",
    "\n",
    "### دستاوردهای پروژه\n",
    "\n",
    "1. **پیاده‌سازی کامل یک سیستم LPR**: از جمع‌آوری داده تا استقرار نهایی\n",
    "2. **حل مشکل پلاک‌های خاص**: با افزایش داده هدفمند\n",
    "3. **بهینه‌سازی برای embedded systems**: اجرا با 1.5 FPS بر روی RPi5\n",
    "4. **معماری یکپارچه**: استفاده از یک مدل برای تمام وظایف\n",
    "\n",
    "### محدودیت‌ها و کارهای آینده\n",
    "\n",
    "1. **سرعت**: FPS بالاتر با استفاده از Coral TPU یا Jetson Nano\n",
    "2. **دقت در شب**: افزودن داده‌های شبانه و استفاده از IR illumination\n",
    "3. **پلاک‌های کثیف**: افزایش robustness با augmentation بیشتر\n",
    "4. **ذخیره‌سازی**: اضافه کردن قابلیت ذخیره تصاویر و لاگ‌گیری\n",
    "\n",
    "### سخن پایانی\n",
    "\n",
    "این پروژه نشان می‌دهد که با استفاده از ابزارهای مدرن machine learning و کمی خلاقیت، می‌توان سیستم‌های practical و کاربردی را حتی با سخت‌افزار محدود پیاده‌سازی کرد. تجربه کار با Raspberry Pi و بهینه‌سازی مدل‌ها برای embedded systems، درس‌های ارزشمندی در مورد trade-off بین دقت، سرعت و منابع محاسباتی به ما آموخت.\n",
    "\n",
    "توضیحات بصورت markdown و همچنین تمیزکردن کد برای خوانایی بیشتر بهمراه کامنت گذاری پس از انجام پروژه توسط هوش مصنوعی تدوین گردیده است."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
